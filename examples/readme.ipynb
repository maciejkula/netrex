{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural networks for recommendations\n",
    "\n",
    "Maciej Kula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation\n",
    "\n",
    "The rise of deep learning has given us new techniques and neural network toolkits which are useful even if your models aren't particularly deep, and do not rely on sophisticated hierarchical representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Flexibility\n",
    "\n",
    "Don't have to painstakingly implement complicated SGD models by hand.\n",
    "\n",
    "Build them up from simple elements instead and let the framework take care of the fitting.\n",
    "\n",
    "If we're doing recommendations, this means we can freely experiment with adding side information and using new loss functions to tailor models to our unique problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### New modelling techniques\n",
    "\n",
    "Secondly, the elements we can use are substantially improve on what was possible before\n",
    "- CNNs for including image data\n",
    "- RNNs for sequence prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this talk, I'm going to show off these qualities by doing three things:\n",
    "- translating a classic matrix factorization model into a neural network toolkit,\n",
    "- quickly building a number of ranking losses to use with it, and\n",
    "- switching out the factorization model for a sequence-based one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Factorization models\n",
    "Take a sparse user-item interaction matrix, and factorize it as a product of two dense matrices of user and item latent vectors.\n",
    "\n",
    "$$\n",
    "I = \\begin{pmatrix}\n",
    "  1.0 & 0.0 & \\cdots & 1.0 \\\\\n",
    "  0.0 & 1.0 & \\cdots & 0.0 \\\\\n",
    "  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "  1.0 & 1.0 & \\cdots & 1.0 \n",
    " \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Famously introduced during the Netflix Prize contest, and still a workhorse in its more modern implicit feedback form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bilinear neural network model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "![title](bilinear.dot.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- two embedding layers, for users and items\n",
    "- connected by a dot product\n",
    "- fit with negative sampling and logistic loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```python\n",
    "class BilinearNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_users, num_items, embedding_dim, sparse=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.user_embeddings = ScaledEmbedding(num_users, embedding_dim,\n",
    "                                               sparse=sparse)\n",
    "        self.item_embeddings = ScaledEmbedding(num_items, embedding_dim,\n",
    "                                               sparse=sparse)\n",
    "        self.user_biases = ZeroEmbedding(num_users, 1, sparse=sparse)\n",
    "        self.item_biases = ZeroEmbedding(num_items, 1, sparse=sparse)\n",
    "\n",
    "    def forward(self, user_ids, item_ids):\n",
    "\n",
    "        # snip\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```python\n",
    "class BilinearNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_users, num_items, embedding_dim, sparse=False):\n",
    "      \n",
    "        # snip\n",
    "\n",
    "    def forward(self, user_ids, item_ids):\n",
    "\n",
    "        user_embedding = self.user_embeddings(user_ids)\n",
    "        item_embedding = self.item_embeddings(item_ids)\n",
    "\n",
    "        user_embedding = user_embedding.view(-1, self.embedding_dim)\n",
    "        item_embedding = item_embedding.view(-1, self.embedding_dim)\n",
    "\n",
    "        user_bias = self.user_biases(user_ids).view(-1, 1)\n",
    "        item_bias = self.item_biases(item_ids).view(-1, 1)\n",
    "\n",
    "        dot = (user_embedding * item_embedding).sum(1)\n",
    "\n",
    "        return dot + user_bias + item_bias\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sampled loss function\n",
    "\n",
    "For the loss, we could output scores for all possible items, trying to predict 1 for all items the user interacted with, and 0 for all other items.\n",
    "\n",
    "In practice use a sampling approach: compute loss for all positive interactions and a sample of missing ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```python\n",
    "    def _pointwise_loss(self, users, items, ratings):\n",
    "\n",
    "        negatives = Variable(\n",
    "            _gpu(\n",
    "                torch.from_numpy(np.random.randint(0,\n",
    "                                                   self._num_items,\n",
    "                                                   len(users))),\n",
    "                self._use_cuda)\n",
    "        )\n",
    "\n",
    "        positives_loss = (1.0 - F.sigmoid(self._net(users, items)))\n",
    "        negatives_loss = F.sigmoid(self._net(users, negatives))\n",
    "\n",
    "        return torch.cat([positives_loss, negatives_loss]).mean()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from netrex.netrex import FactorizationModel, SequenceModel, generate_sequences\n",
    "from netrex.evaluation import auc_score, mrr_score\n",
    "\n",
    "from lightfm.datasets import fetch_movielens\n",
    "\n",
    "def _binarize(dataset):\n",
    "\n",
    "    dataset = dataset.copy()\n",
    "\n",
    "    dataset.data = (dataset.data >= 0.0).astype(np.float32)\n",
    "    dataset = dataset.tocsr()\n",
    "    dataset.eliminate_zeros()\n",
    "\n",
    "    return dataset.tocoo()\n",
    "\n",
    "movielens = fetch_movielens()\n",
    "ratings_train, ratings_test = movielens['train'], movielens['test']\n",
    "train, test = _binarize(movielens['train']), _binarize(movielens['test'])\n",
    "\n",
    "embedding_dim = 128\n",
    "minibatch_size = 1024\n",
    "n_iter = 10\n",
    "cuda = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR score: 0.06\n"
     ]
    }
   ],
   "source": [
    "model = FactorizationModel(loss='pointwise',\n",
    "                           n_iter=n_iter,\n",
    "                           embedding_dim=embedding_dim,\n",
    "                           batch_size=minibatch_size,\n",
    "                           use_cuda=cuda)\n",
    "\n",
    "model.fit(train)\n",
    "\n",
    "print('MRR score: {:.2f}'.format(mrr_score(model, test, train).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ranking losses\n",
    "This model is good, but we can do better with a ranking loss.\n",
    "\n",
    " $$\n",
    " L = 1.0 - \\mathrm{sigmoid}(\\mathrm{positive} - \\mathrm{negative})\n",
    " $$\n",
    "\n",
    " - Construct triplets of (user, positive, negative item)\n",
    " - Positive item should be ranked higher than negative item."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Use a siamese network\n",
    "\n",
    "- re-use the embedding representation from before\n",
    "- compute the score for the positive and negative item using shared embedding layers\n",
    "- feed it into the pairwise loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Use a siamese network\n",
    "\n",
    "![Siamese](bilinear_siamese.dot.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Use a siamese network\n",
    "\n",
    "```python\n",
    "    def _bpr_loss(self, users, items, ratings):\n",
    "\n",
    "        negatives = Variable(\n",
    "            _gpu(\n",
    "                torch.from_numpy(np.random.randint(0,\n",
    "                                                   self._num_items,\n",
    "                                                   len(users))),\n",
    "                self._use_cuda)\n",
    "        )\n",
    "\n",
    "        return (1.0 - F.sigmoid(self._net(users, items) -\n",
    "                                self._net(users, negatives))).mean()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR score: 0.08\n"
     ]
    }
   ],
   "source": [
    "model = FactorizationModel(loss='bpr',\n",
    "                           n_iter=n_iter,\n",
    "                           embedding_dim=embedding_dim,\n",
    "                           batch_size=minibatch_size,\n",
    "                           use_cuda=cuda)\n",
    "\n",
    "model.fit(train)\n",
    "\n",
    "print('MRR score: {:.2f}'.format(mrr_score(model, test, train).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Adaptive ranking loss\n",
    "That's an improvement, but we can do even better.\n",
    "\n",
    "Sample a couple of negative examples, and pick the ones that are closes to violating the correct ranking in your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Adaptive ranking loss\n",
    "```python\n",
    "    def _adaptive_loss(self, users, items, ratings):\n",
    "\n",
    "        negative_predictions = []\n",
    "\n",
    "        for _ in range(5):\n",
    "            negatives = Variable(\n",
    "                _gpu(\n",
    "                    torch.from_numpy(np.random.randint(0,\n",
    "                                                       self._num_items,\n",
    "                                                       len(users))),\n",
    "                    self._use_cuda)\n",
    "            )\n",
    "\n",
    "            negative_predictions.append(self._net(users, negatives))\n",
    "\n",
    "        best_negative_prediction, _ = torch.cat(negative_predictions, 1).max(1)\n",
    "        positive_prediction = self._net(users, items)\n",
    "\n",
    "        return torch.mean(torch.clamp(best_negative_prediction -\n",
    "                                      positive_prediction\n",
    "                                      + 1.0, 0.0))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR score: 0.11\n"
     ]
    }
   ],
   "source": [
    "model = FactorizationModel(loss='adaptive',\n",
    "                           n_iter=n_iter,\n",
    "                           embedding_dim=embedding_dim,\n",
    "                           batch_size=minibatch_size,\n",
    "                           use_cuda=cuda)\n",
    "\n",
    "model.fit(train)\n",
    "\n",
    "print('MRR score: {:.2f}'.format(mrr_score(model, test, train).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Flexibility\n",
    "Same representation, a number of ranking losses.\n",
    "\n",
    "Trivial to implement.\n",
    "\n",
    "We can now keep the loss functions but switch out the representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recommendations as sequence prediction\n",
    "\n",
    "Users interact with items in an approximately sequential manner.\n",
    "\n",
    "We can treat recommending items for a given user as trying to predict the next item in a sequence.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If a user bought items `a`, `b`, `c`, and `d`, we'd like to predict \n",
    "\n",
    "```python\n",
    "[a] -> b\n",
    "[a, b] -> c\n",
    "[a, b, c] -> d\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sequence-based user representation\n",
    "\n",
    "There is a number of ways of doing so, but they all consist in representing the user as a function of the items they've bought in the past."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The user representation can be:\n",
    "\n",
    "- the average of the item vectors that they bought before ([Covington et al.](https://research.google.com/pubs/pub45530.html))\n",
    "- a RNN ([Hidasi et al.](https://arxiv.org/pdf/1511.06939.pdf))\n",
    "- a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advantages\n",
    "- captures sequential information\n",
    "- less pressing need to retrain: predictions automatically adapt as user purchase history grows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Disadvantages:\n",
    "- model slower to evaluate\n",
    "- smaller benefit if data has no clear sequential structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### User representation vs loss function\n",
    "\n",
    "Note that user representation is separate from the loss used, so we can use the BPR and adaptive ranking losses with sequence-based models too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data preparation\n",
    "\n",
    "Transform data into left-padded sequences. `[a, b, c, d]` becomes:\n",
    "    \n",
    "```python\n",
    "[0, a, b, c] -> [a, b, c, d]\n",
    "```\n",
    "\n",
    "This is a single row of the input matrix, but expresses four predictions: \n",
    "```python\n",
    "[0] -> a\n",
    "[0, a] -> b\n",
    "[0, a, b] -> c\n",
    "[0, a, b, c] -> d\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Could also split subsequences into separate training examples, making the input matrix\n",
    "```python\n",
    "[0, 0, 0, 0] -> a\n",
    "[0, 0, 0, a] -> b\n",
    "[0, 0, a, b] -> c\n",
    "[0, a, b, c] -> d\n",
    "```\n",
    "This is more flexible, but requires computing outputs of the RNN multiple times over, making fitting slower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Representations\n",
    "We're going to try a pooling-based and a recurrent representation (as well as a popularity baseline to make sure we're learning anything at all!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "```python\n",
    "class PoolNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_items, embedding_dim, sparse=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.item_embeddings = ScaledEmbedding(num_items, embedding_dim,\n",
    "                                               sparse=sparse,\n",
    "                                               padding_idx=0)\n",
    "        self.item_biases = ZeroEmbedding(num_items, 1, sparse=sparse,\n",
    "                                         padding_idx=0)\n",
    "\n",
    "    def forward(self, item_sequences, item_ids):\n",
    "\n",
    "        target_embedding = self.item_embeddings(item_ids)\n",
    "        user_representations = torch.cumsum(\n",
    "            self.item_embeddings(item_sequences),\n",
    "            1\n",
    "        )\n",
    "\n",
    "        target_bias = self.item_biases(item_ids)\n",
    "\n",
    "        dot = (user_representations * target_embedding).sum(2)\n",
    "\n",
    "        return dot + target_bias\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```python\n",
    "class LSTMNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_items, embedding_dim, sparse=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.item_embeddings = ScaledEmbedding(num_items, embedding_dim,\n",
    "                                               sparse=sparse,\n",
    "                                               padding_idx=0)\n",
    "        self.item_biases = ZeroEmbedding(num_items, 1, sparse=sparse,\n",
    "                                         padding_idx=0)\n",
    "\n",
    "        self.lstm = nn.LSTM(batch_first=True,\n",
    "                            input_size=embedding_dim,\n",
    "                            hidden_size=embedding_dim)\n",
    "\n",
    "    def forward(self, item_sequences, item_ids):\n",
    "\n",
    "        target_embedding = self.item_embeddings(item_ids)\n",
    "        user_representations, _ = self.lstm(\n",
    "            self.item_embeddings(item_sequences)\n",
    "        )\n",
    "        target_bias = self.item_biases(item_ids)\n",
    "\n",
    "        dot = (user_representations * target_embedding).sum(2)\n",
    "\n",
    "        return dot + target_bias\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's run this!\n",
    "\n",
    "Load the MovieLens data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting from [0, 130, 190, 31, 28] to [130, 190, 31, 28, 52]\n"
     ]
    }
   ],
   "source": [
    "from netrex import rnn_data\n",
    "from netrex.netrex import FactorizationModel, SequenceModel, generate_sequences\n",
    "\n",
    "sequence_data = rnn_data.fetch_movielens()\n",
    "train_sequences, train_targets = generate_sequences(sequence_data['train'])\n",
    "test_sequences, test_targets = generate_sequences(sequence_data['test'])\n",
    "\n",
    "print('Predicting from [{}] to [{}]'.format(\n",
    "    ', '.join((str(x) for x in train_sequences[1, :5])),\n",
    "    ', '.join((str(x) for x in train_targets[1, :5]))\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Set up the models, starting with a popularity baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "minibatch_size = 64\n",
    "n_iter = 10\n",
    "cuda = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR on training set 0.12\n",
      "MRR on test set 0.12\n"
     ]
    }
   ],
   "source": [
    "model = SequenceModel(loss='bpr',\n",
    "                      representation='popularity',\n",
    "                      n_iter=n_iter,\n",
    "                      embedding_dim=embedding_dim,\n",
    "                      batch_size=minibatch_size,\n",
    "                      use_cuda=cuda)\n",
    "\n",
    "model.fit(train_sequences, train_targets, verbose=False)\n",
    "\n",
    "print('MRR on training set {:.2f}'.format(\n",
    "    model.compute_mrr(train_sequences, train_targets, num_samples=200).mean()\n",
    "))\n",
    "print('MRR on test set {:.2f}'.format(\n",
    "    model.compute_mrr(test_sequences, test_targets, num_samples=200).mean()\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR on training set 0.17\n",
      "MRR on test set 0.14\n"
     ]
    }
   ],
   "source": [
    "model = SequenceModel(loss='bpr',\n",
    "                      representation='pool',\n",
    "                      n_iter=n_iter,\n",
    "                      embedding_dim=embedding_dim,\n",
    "                      batch_size=minibatch_size,\n",
    "                      use_cuda=cuda)\n",
    "\n",
    "model.fit(train_sequences, train_targets, verbose=False)\n",
    "\n",
    "print('MRR on training set {:.2f}'.format(\n",
    "    model.compute_mrr(train_sequences, train_targets, num_samples=200).mean()\n",
    "))\n",
    "print('MRR on test set {:.2f}'.format(\n",
    "    model.compute_mrr(test_sequences, test_targets, num_samples=200).mean()\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR on training set 0.12\n",
      "MRR on test set 0.11\n"
     ]
    }
   ],
   "source": [
    "model = SequenceModel(loss='bpr',\n",
    "                      representation='lstm',\n",
    "                      n_iter=n_iter,\n",
    "                      embedding_dim=embedding_dim,\n",
    "                      batch_size=minibatch_size,\n",
    "                      use_cuda=cuda)\n",
    "\n",
    "model.fit(train_sequences, train_targets, verbose=False)\n",
    "\n",
    "print('MRR on training set {:.2f}'.format(\n",
    "    model.compute_mrr(train_sequences, train_targets, num_samples=200).mean()\n",
    "))\n",
    "print('MRR on test set {:.2f}'.format(\n",
    "    model.compute_mrr(test_sequences, test_targets, num_samples=200).mean()\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Oops.\n",
    "\n",
    "LSTMs beat popularity (yay!) but perform worse than pooling.\n",
    "- bad hyperparameters?\n",
    "- sequence information not important?\n",
    "- did we get the model architecture wrong?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sanity check\n",
    "Generate data with obvious sequential strucure and try again.\n",
    "\n",
    "Use number sequences: `[0, 1, 2, 3, ...] -> [1, 2, 3, 4, ...]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def generate_sequential_interactions(sequence_len, num_rows):\n",
    "\n",
    "    return sp.csr_matrix(np.tile(np.arange(sequence_len) + 1, (num_rows, 1)))\n",
    "\n",
    "train_sequences, train_targets = generate_sequences(generate_sequential_interactions(20, 1000))\n",
    "test_sequences, test_targets = generate_sequences(generate_sequential_interactions(20, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR on training set 0.07\n",
      "MRR on test set 0.07\n"
     ]
    }
   ],
   "source": [
    "model = SequenceModel(loss='bpr',\n",
    "                      representation='popularity',\n",
    "                      n_iter=n_iter,\n",
    "                      embedding_dim=embedding_dim,\n",
    "                      batch_size=minibatch_size,\n",
    "                      use_cuda=cuda)\n",
    "\n",
    "model.fit(train_sequences, train_targets, verbose=False)\n",
    "\n",
    "print('MRR on training set {:.2f}'.format(\n",
    "    model.compute_mrr(train_sequences, train_targets, num_samples=200).mean()\n",
    "))\n",
    "print('MRR on test set {:.2f}'.format(\n",
    "    model.compute_mrr(test_sequences, test_targets, num_samples=200).mean()\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR on training set 0.13\n",
      "MRR on test set 0.13\n"
     ]
    }
   ],
   "source": [
    "model = SequenceModel(loss='bpr',\n",
    "                      representation='pool',\n",
    "                      n_iter=n_iter,\n",
    "                      embedding_dim=embedding_dim,\n",
    "                      batch_size=minibatch_size,\n",
    "                      use_cuda=cuda)\n",
    "\n",
    "model.fit(train_sequences, train_targets, verbose=False)\n",
    "\n",
    "print('MRR on training set {:.2f}'.format(\n",
    "    model.compute_mrr(train_sequences, train_targets, num_samples=200).mean()\n",
    "))\n",
    "print('MRR on test set {:.2f}'.format(\n",
    "    model.compute_mrr(test_sequences, test_targets, num_samples=200).mean()\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR on training set 0.77\n",
      "MRR on test set 0.77\n"
     ]
    }
   ],
   "source": [
    "model = SequenceModel(loss='bpr',\n",
    "                      representation='lstm',\n",
    "                      n_iter=n_iter,\n",
    "                      embedding_dim=embedding_dim,\n",
    "                      batch_size=minibatch_size,\n",
    "                      use_cuda=cuda)\n",
    "\n",
    "model.fit(train_sequences, train_targets, verbose=False)\n",
    "\n",
    "print('MRR on training set {:.2f}'.format(\n",
    "    model.compute_mrr(train_sequences, train_targets, num_samples=200).mean()\n",
    "))\n",
    "print('MRR on test set {:.2f}'.format(\n",
    "    model.compute_mrr(test_sequences, test_targets, num_samples=200).mean()\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sanity check\n",
    "\n",
    "Fantastic: LSTMs do give us great results on dataset where the sequential nature of interactions is important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "Amazing flexibility.\n",
    "\n",
    "Powerful building blocks.\n",
    "\n",
    "Speed of iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "Is everything perfect? No.\n",
    "\n",
    "Fitting NN models on sparse problems can be slow.\n",
    "- use sparse gradients on the CPU?\n",
    "- run on GPU with huge minibatches?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
